{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ykviXcOcrg9"
   },
   "source": [
    "## Research And Generate\n",
    "RAG System written using Ollama and LLangchain Libraries\n",
    "\n",
    "Use this runtime for online testing\n",
    "https://colab.research.google.com/drive/1s9tpx736ohbZu0STumiba3-noPSV0Wdg?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjaZuxeadnSP"
   },
   "source": [
    "1. Installing Required Libraries, in this case llangchain for Enconding stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 5655,
     "status": "ok",
     "timestamp": 1740313942583,
     "user": {
      "displayName": "Zerone Brianstorm",
      "userId": "05377539483299604687"
     },
     "user_tz": -330
    },
    "id": "_2_bHEUociX-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain-community (/home/zerone/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-community (/home/zerone/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-community (/home/zerone/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution -angchain-community (/home/zerone/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-community (/home/zerone/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-community (/home/zerone/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-community (/home/zerone/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain-community (/home/zerone/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet  langchain langchain-community langchainhub gpt4all langchain-chroma pypdf\n",
    "!pip install -U langchain-chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYWyqbuwd0J9"
   },
   "source": [
    "Importing Installed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1436,
     "status": "ok",
     "timestamp": 1740313946445,
     "user": {
      "displayName": "Zerone Brianstorm",
      "userId": "05377539483299604687"
     },
     "user_tz": -330
    },
    "id": "9Ey7RPDXdzJt",
    "outputId": "4be74eb4-29be-4f11-b9c5-be65b2da4939"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15617/726452791.py:7: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma()\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "vectorstore = Chroma()\n",
    "vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UgptA3Ad_fT"
   },
   "source": [
    "Loading an exmaple Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1488,
     "status": "ok",
     "timestamp": 1740313950923,
     "user": {
      "displayName": "Zerone Brianstorm",
      "userId": "05377539483299604687"
     },
     "user_tz": -330
    },
    "id": "sejRPO0EeIOa"
   },
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"nsbm_foc_data.pdf\")\n",
    "pages = []\n",
    "\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGMI6wacevzd"
   },
   "source": [
    "Splitting the downloaded data and storing them in a vectorized database object, Doing this only once is enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6861,
     "status": "ok",
     "timestamp": 1740313960151,
     "user": {
      "displayName": "Zerone Brianstorm",
      "userId": "05377539483299604687"
     },
     "user_tz": -330
    },
    "id": "Er0cyk5aez19"
   },
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=pages, embedding=GPT4AllEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nz2PGwrCe7xD"
   },
   "source": [
    "Testing similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 143,
     "status": "ok",
     "timestamp": 1740313961501,
     "user": {
      "displayName": "Zerone Brianstorm",
      "userId": "05377539483299604687"
     },
     "user_tz": -330
    },
    "id": "wH9TEI0ze8hd",
    "outputId": "37aee0a6-bb3a-4a7e-b3be-40829b710754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the dean's message?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVdpgFIUgbFd"
   },
   "source": [
    "### *Okay  now the Research part is done*\n",
    "Lets focus on generating part now\n",
    "\n",
    "Installing libraries required to run a LLM locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1770284,
     "status": "ok",
     "timestamp": 1740315733706,
     "user": {
      "displayName": "Zerone Brianstorm",
      "userId": "05377539483299604687"
     },
     "user_tz": -330
    },
    "id": "fE-ndB5Nr12l",
    "outputId": "4b9aeb1e-077a-41f6-aca2-7d688b4427cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 08:25:36.037779: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/zerone/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/home/zerone/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Summerizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "# text = \"\"\"\n",
    "# NSBM (National School of Business Management) is a leading university located in Sri Lanka. It offers a range of undergraduate and postgraduate programs in various fields such as Business, Information Technology, and Engineering. \n",
    "# The faculty is composed of highly qualified professionals who are experts in their respective fields. Students benefit from state-of-the-art facilities, interactive learning methods, and a diverse cultural environment. \n",
    "# The university also emphasizes industry collaborations, providing students with ample opportunities for internships and job placements after graduation.\n",
    "# \"\"\"\n",
    "\n",
    "# # Get the summary of the text\n",
    "# summary = summarizer(docs, max_length=150, min_length=50, do_sample=False)\n",
    "\n",
    "# # Output the summary\n",
    "# print(\"Summary:\", summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 93,
     "status": "ok",
     "timestamp": 1740315916275,
     "user": {
      "displayName": "Zerone Brianstorm",
      "userId": "05377539483299604687"
     },
     "user_tz": -330
    },
    "id": "u_Kn67fch0gq"
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xclY5wnUh6Li"
   },
   "source": [
    "Let's Download the model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 447,
     "status": "ok",
     "timestamp": 1740312031202,
     "user": {
      "displayName": "Zerone Brianstorm",
      "userId": "05377539483299604687"
     },
     "user_tz": -330
    },
    "id": "qOIlXNNLkxSV",
    "outputId": "bdf9e1ec-2699-4f63-8140-6b2d4e966b9d"
   },
   "outputs": [],
   "source": [
    "!ls\n",
    "!nvidia-smi\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bLd1OkCk2vU"
   },
   "source": [
    "Setting the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57039,
     "status": "ok",
     "timestamp": 1740312504401,
     "user": {
      "displayName": "Zerone Brianstorm",
      "userId": "05377539483299604687"
     },
     "user_tz": -330
    },
    "id": "KvZpTlbik8cv",
    "outputId": "033b5c18-f0b3-461c-c48e-cb21ce6538d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerone/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"cognitivecomputations/Dolphin3.0-Llama3.2-3B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,  device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Combine the context and the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "token_counts = sum([len(tokenizer.encode(doc.page_content)) for doc in docs])\n",
    "\n",
    "# According to the internet, the maximum token count of LLAMA3.2 is 4096, \n",
    "# So just to be safe, lets truncate the context input to 2000 tokens\n",
    "print(token_counts)\n",
    "\n",
    "# Truncate context to 2000 tokens\n",
    "tokens = tokenizer.encode(context, add_special_tokens=False)\n",
    "truncated_tokens = tokens[:2000]\n",
    "context = tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n",
    "\n",
    "prompt = f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output = model.generate(**input_ids, max_new_tokens=4000)\n",
    "answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Memory with Langchain [TODO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "def call_model(state: MessageState):\n",
    "    system_pormpt = (\n",
    "        \"You are a helpful assistant\",\n",
    "        \"Your name is NSBM Chat\"\n",
    "    )\n",
    "    message = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n",
    "    response = mode.invoke(message)\n",
    "    return {\"message\": message}\n",
    "\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model's Capabilities with ROUGE Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from rouge_score import rouge_scorer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def initialize_vectorstore():\n",
    "    \"\"\"Initialize ChromaDB\"\"\"\n",
    "    vectorstore = Chroma()\n",
    "    return vectorstore\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    \"\"\"Load the PDF\"\"\"\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load()\n",
    "    return pages\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load the Model.\"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "def process_text(pages, tokenizer, max_tokens=2000):\n",
    "    \"\"\"Truncating\"\"\"\n",
    "    context = \"\\n\".join([page.page_content for page in pages])\n",
    "    tokens = tokenizer.encode(context, add_special_tokens=False, truncation=True, max_length=max_tokens)\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "def generate_answer(model, tokenizer, context, question):\n",
    "    \"\"\"Generate response\"\"\"\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    output = model.generate(input_ids, max_new_tokens=400)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True).split(\"Answer:\")[-1].strip()\n",
    "\n",
    "def ROUGE(reference, gpt):\n",
    "    \"\"\"Calculate ROUGE score\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    return scorer.score(reference, gpt)\n",
    "\n",
    "def main():\n",
    "    vectorstore = initialize_vectorstore()\n",
    "    pages = load_pdf(\"nsbm_foc_data.pdf\")\n",
    "    model, tokenizer = load_model(\"cognitivecomputations/Dolphin3.0-Llama3.2-3B\")\n",
    "    context = process_text(pages, tokenizer)\n",
    "\n",
    "    questionArray = [\n",
    "        \"Who is the Dean of NSBM FOC?\",\n",
    "        \"How to contact NSBM for new Enrollments?\",\n",
    "        \"What is the dean's message about? Answer in a paragraph\"\n",
    "    ]\n",
    "    \n",
    "    referenceAnswerArray = [\n",
    "        \"Dean of the NSBM's Faculty of Computing is Dr. Rasika Ranaweera.\",\n",
    "        \"Send an email to inquiries@nsbm.ac.lk.\",\n",
    "        \"\"\"The Dean's message welcomes students to the Faculty of Computing at NSBM Green University, emphasizing innovative and industry-relevant education. \n",
    "        It highlights the well-qualified faculty and strong student support services aimed at preparing students for successful careers. \n",
    "        The message encourages students to work hard, set high standards, and take responsibility for their learning. Ultimately, it expresses the university’s goal \n",
    "        of developing well-rounded individuals who will succeed in life and serve as ambassadors for NSBM.\"\"\"\n",
    "    ]\n",
    "\n",
    "    gptAnswerArray = []\n",
    "    scoreArray = []\n",
    "\n",
    "    for question in questionArray:\n",
    "        gptAnswer = generate_answer(model, tokenizer, context, question)\n",
    "        gptAnswerArray.append(gptAnswer)\n",
    "\n",
    "    for i, (reference, gpt) in enumerate(zip(referenceAnswerArray, gptAnswerArray)):\n",
    "        print(f\"Calculating ROUGE Scores for Question {i+1}\")\n",
    "        print(\"Reference:\", reference)\n",
    "        print(\"GPT:\", gpt)\n",
    "        scores = ROUGE(reference, gpt)\n",
    "        print(f\"Scores: {scores}\")\n",
    "        print(\"-----------------------------\")\n",
    "    print(\"Testing Completed !  Displaying Scores...\")\n",
    "    for i, score in enumerate(scoreArray):\n",
    "        print(\"----------------------------\")\n",
    "        print(f\"Scores for Question {i+1}:\")\n",
    "        print(f\"Question= {questionArray[i]}\")\n",
    "        print(f\"Reference Answer = {referenceAnswerArray[i]}\")\n",
    "        print(f\"Model Answer = {gptAnswerArray[i]}\")\n",
    "        print(f\"ROUGE Scores: {score}\")\n",
    "        print(\"-----------------------------\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyORhkf9JEi/MsK/LchuVo3R",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
